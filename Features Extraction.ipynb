{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction\n",
    "\n",
    "In this notebook we demonstrate how to encode features into machine-readable representation (i.e. numeric vectors).\n",
    "\n",
    "We first build vocabularies based on word frequences and character grams and then apply pre-processing over our dataset and map the examples to the vocabularies into feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# import the pre-processing functions\n",
    "from processing import text as text_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the comments dataset\n",
    "comments = pd.read_csv('datasets/labeled_comments.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159686, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"This:NEWLINE_TOKEN:One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion. NEWLINE_TOKENsounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful.NEWLINE_TOKENNEWLINE_TOKENThe paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SRNEWLINE_TOKEN\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['comment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove special tokens\n",
    "comments['comment'] = comments['comment'].str.replace(u'NEWLINE_TOKEN|TAB_TOKEN', u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"This: :One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion.  sounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful.  The paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SR \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['comment'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Vector\n",
    "\n",
    "Build the word frequences vocabulary and the function which apply pre-processing on a given text and maps it to the semantic vocabulary to produce the semantic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-processing pipeline\n",
    "pipeline = [\n",
    "    # convert all letters to lowercase\n",
    "    text_prepro.to_lower,\n",
    "    # transliterate non-english letters\n",
    "    text_prepro.transliterate,\n",
    "    # strip tags (@ and #) from words\n",
    "    text_prepro.remove_tags,\n",
    "    # tokenize URLs into \"__URL__\"\n",
    "    text_prepro.tokenize_url,\n",
    "    # Keep alphanumeric characters only\n",
    "    text_prepro.alphanum\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hëllo @foobar, VISIT my [site](http://foo.bar) #thankyou!\n",
      "[1]: hëllo @foobar, visit my [site](http://foo.bar) #thankyou!\n",
      "[2]: hello @foobar, visit my [site](http://foo.bar) #thankyou!\n",
      "[3]: hello foobar, visit my [site](http://foo.bar) thankyou!\n",
      "[4]: hello foobar, visit my [site](__URL__) thankyou!\n",
      "[5]: hello foobar visit my site __URL__ thankyou\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = u'Hëllo @foobar, VISIT my [site](http://foo.bar) #thankyou!'\n",
    "print text\n",
    "for i, pipe in enumerate(pipeline, 1):\n",
    "    text = pipe(text)\n",
    "    print u'[{}]: {}'.format(i, text)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the prepro as a functiom\n",
    "def semantic_prepro(text):\n",
    "    for pipe in pipeline:\n",
    "        text = pipe(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract word counts\n",
    "word_counts = Counter()\n",
    "for comment in comments['comment']:\n",
    "    comment = semantic_prepro(comment)\n",
    "    word_counts.update(comment.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180929\n"
     ]
    }
   ],
   "source": [
    "print len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 499009),\n",
       " (u'to', 299282),\n",
       " (u'i', 241781),\n",
       " (u'and', 225667),\n",
       " (u'of', 225664),\n",
       " (u'you', 220267),\n",
       " (u'a', 217139),\n",
       " (u'is', 177412),\n",
       " (u'that', 161865),\n",
       " (u'it', 149423),\n",
       " (u'in', 146057),\n",
       " (u'for', 103492),\n",
       " (u'this', 98202),\n",
       " (u'not', 94392),\n",
       " (u'on', 90560),\n",
       " (u'be', 84128),\n",
       " (u'as', 77874),\n",
       " (u'are', 72731),\n",
       " (u'have', 72649),\n",
       " (u's', 72443),\n",
       " (u'your', 63192),\n",
       " (u'with', 60039),\n",
       " (u't', 59801),\n",
       " (u'if', 59174),\n",
       " (u'article', 57902),\n",
       " (u'was', 54862),\n",
       " (u'or', 53876),\n",
       " (u'but', 51469),\n",
       " (u'wikipedia', 46715),\n",
       " (u'page', 46352),\n",
       " (u'my', 45497),\n",
       " (u'an', 45224),\n",
       " (u'from', 41809),\n",
       " (u'by', 41630),\n",
       " (u'do', 40361),\n",
       " (u'can', 39789),\n",
       " (u'at', 39736),\n",
       " (u'about', 37335),\n",
       " (u'so', 36840),\n",
       " (u'me', 36745),\n",
       " (u'what', 35582),\n",
       " (u'there', 35494),\n",
       " (u'all', 31818),\n",
       " (u'has', 31067),\n",
       " (u'will', 30812),\n",
       " (u'please', 30209),\n",
       " (u'he', 29569),\n",
       " (u'would', 29547),\n",
       " (u'they', 29479),\n",
       " (u'no', 29465)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 words\n",
    "word_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'leaded', 1),\n",
       " (u'shemeet', 1),\n",
       " (u'pe\\u026al\\u0268n', 1),\n",
       " (u'hmmpff', 1),\n",
       " (u'qoyunli', 1),\n",
       " (u'thoroughfare', 1),\n",
       " (u'fradaulent', 1),\n",
       " (u'shty', 1),\n",
       " (u'proberly', 1),\n",
       " (u'pocketbook', 1),\n",
       " (u'mahakavyas', 1),\n",
       " (u'fudd', 1),\n",
       " (u'cryokinesis', 1),\n",
       " (u'wonk', 1),\n",
       " (u'sipopo', 1),\n",
       " (u'belembay', 1),\n",
       " (u'knisfo', 1),\n",
       " (u'onclelosse', 1),\n",
       " (u'pertecting', 1),\n",
       " (u'antivermins', 1),\n",
       " (u'warrig', 1),\n",
       " (u'ajna', 1),\n",
       " (u'talkapge', 1),\n",
       " (u'nepotising', 1),\n",
       " (u'rattner2', 1),\n",
       " (u'bratwurst', 1),\n",
       " (u'publicationthe', 1),\n",
       " (u'clarityafflicting', 1),\n",
       " (u'ornella', 1),\n",
       " (u'cronyn', 1),\n",
       " (u'australianist', 1),\n",
       " (u'chromate', 1),\n",
       " (u'ehlers', 1),\n",
       " (u'spanko', 1),\n",
       " (u'thurst', 1),\n",
       " (u'gnawing', 1),\n",
       " (u'bennies', 1),\n",
       " (u'spanky', 1),\n",
       " (u'as_of', 1),\n",
       " (u'branco', 1),\n",
       " (u'\\u65b0\\u64b0\\u59d3\\u6c0f\\u9332', 1),\n",
       " (u'accoutns', 1),\n",
       " (u'queensborough', 1),\n",
       " (u'commagene', 1),\n",
       " (u'personal_attacks_', 1),\n",
       " (u'psone', 1),\n",
       " (u'fapped', 1),\n",
       " (u'classsssssssss', 1),\n",
       " (u'morihiro', 1),\n",
       " (u'downstep', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bottom 10 words\n",
    "word_counts.most_common()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select words with more than 1 occurrence\n",
    "select = {k: v for k, v in word_counts.iteritems() if v > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87348"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign unique indexes to each word\n",
    "sorted_words = sorted(select.iteritems(), key=lambda (k, v): (v, k), reverse=True)\n",
    "word_indexes = {k: i for i, (k, _) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes['damn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# semantic vector mapper\n",
    "def semantic_vector(text):\n",
    "    for pipe in pipeline:\n",
    "        text = pipe(text)\n",
    "    vector = np.zeros((len(word_indexes),), dtype=np.float32)\n",
    "    for w in text.split():\n",
    "        ind = word_indexes.get(w)\n",
    "        if ind is not None:\n",
    "            vector[ind] = 1.\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\" ::::That's reasonable enough; I just saw the conflict on this one page and not on the rest.  I don't know details of it either, and if I had noticed the rest I probably would have been suspicious also (like Mav, below).  Also like mav, though, I probably would have noted why I reverted any changes on the article's talk page.  Best,   \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text\n",
    "example = comments['comment'][30]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate vector\n",
    "vector_words = semantic_vector(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87348,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_words[word_indexes['enough']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = semantic_prepro(example).split()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter vector\n",
    "\n",
    "We generate a list of 1-gram characters from alphanumeric characters (0-9, a-z) and punctuations then map a given text's characters to this list to produce the letter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import itertools\n",
    "\n",
    "# character 1-grams (letters, numbers and whitespace)\n",
    "chars = list(set((string.letters + string.digits + string.punctuation).lower() + ' '))\n",
    "char_indexes = {c: i for i, c in enumerate(sorted(chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '#': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " '*': 10,\n",
       " '+': 11,\n",
       " ',': 12,\n",
       " '-': 13,\n",
       " '.': 14,\n",
       " '/': 15,\n",
       " '0': 16,\n",
       " '1': 17,\n",
       " '2': 18,\n",
       " '3': 19,\n",
       " '4': 20,\n",
       " '5': 21,\n",
       " '6': 22,\n",
       " '7': 23,\n",
       " '8': 24,\n",
       " '9': 25,\n",
       " ':': 26,\n",
       " ';': 27,\n",
       " '<': 28,\n",
       " '=': 29,\n",
       " '>': 30,\n",
       " '?': 31,\n",
       " '@': 32,\n",
       " '[': 33,\n",
       " '\\\\': 34,\n",
       " ']': 35,\n",
       " '^': 36,\n",
       " '_': 37,\n",
       " '`': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64,\n",
       " '{': 65,\n",
       " '|': 66,\n",
       " '}': 67,\n",
       " '~': 68}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# character mapper\n",
    "def letter_vector(text):\n",
    "    vector = np.zeros((len(char_indexes),), dtype=np.float32)\n",
    "    for c in text:\n",
    "        i = char_indexes.get(c)\n",
    "        if i is not None:\n",
    "            vector[i] = 1.\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_letters = letter_vector(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate features\n",
    "\n",
    "Finally, after computing each feature for each example, we concatenate them into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_vector = np.concatenate([vector_words, vector_letters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87417,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87417"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_words) + len(vector_letters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
