{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction\n",
    "\n",
    "In this notebook we demonstrate how to encode features into machine-readable representation (i.e. numeric vectors).\n",
    "\n",
    "We first build vocabularies based on word frequences and character grams and then apply pre-processing over our dataset and map the examples to the vocabularies into feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# import the pre-processing functions\n",
    "from processing import text as text_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the comments dataset\n",
    "comments = pd.read_csv('datasets/labeled_comments.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159686, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"This:NEWLINE_TOKEN:One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion. NEWLINE_TOKENsounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful.NEWLINE_TOKENNEWLINE_TOKENThe paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SRNEWLINE_TOKEN\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['comment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove special tokens\n",
    "comments['comment'] = comments['comment'].str.replace(u'NEWLINE_TOKEN|TAB_TOKEN', u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"This: :One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion.  sounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful.  The paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SR \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['comment'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Vector\n",
    "\n",
    "Build the word frequences vocabulary and the function which apply pre-processing on a given text and maps it to the semantic vocabulary to produce the semantic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-processing pipeline\n",
    "pipeline = [\n",
    "    # convert all letters to lowercase\n",
    "    text_prepro.to_lower,\n",
    "    # transliterate non-english letters\n",
    "    text_prepro.transliterate,\n",
    "    # strip tags (@ and #) from words\n",
    "    text_prepro.remove_tags,\n",
    "    # tokenize URLs into \"__URL__\"\n",
    "    text_prepro.tokenize_url,\n",
    "    # Keep alphanumeric characters only\n",
    "    text_prepro.alphanum\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hëllo @foobar, VISIT my [site](http://foo.bar) #thankyou!\n",
      "[1]: hëllo @foobar, visit my [site](http://foo.bar) #thankyou!\n",
      "[2]: hello @foobar, visit my [site](http://foo.bar) #thankyou!\n",
      "[3]: hello foobar, visit my [site](http://foo.bar) thankyou!\n",
      "[4]: hello foobar, visit my [site](__URL__) thankyou!\n",
      "[5]: hello foobar visit my site __URL__ thankyou\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = u'Hëllo @foobar, VISIT my [site](http://foo.bar) #thankyou!'\n",
    "print text\n",
    "for i, pipe in enumerate(pipeline, 1):\n",
    "    text = pipe(text)\n",
    "    print u'[{}]: {}'.format(i, text)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the prepro as a functiom\n",
    "def semantic_prepro(text):\n",
    "    for pipe in pipeline:\n",
    "        text = pipe(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract word counts\n",
    "word_counts = Counter()\n",
    "for comment in comments['comment']:\n",
    "    comment = semantic_prepro(comment)\n",
    "    word_counts.update(comment.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180929\n"
     ]
    }
   ],
   "source": [
    "print len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 499009),\n",
       " (u'to', 299282),\n",
       " (u'i', 241781),\n",
       " (u'and', 225667),\n",
       " (u'of', 225664),\n",
       " (u'you', 220267),\n",
       " (u'a', 217139),\n",
       " (u'is', 177412),\n",
       " (u'that', 161865),\n",
       " (u'it', 149423),\n",
       " (u'in', 146057),\n",
       " (u'for', 103492),\n",
       " (u'this', 98202),\n",
       " (u'not', 94392),\n",
       " (u'on', 90560),\n",
       " (u'be', 84128),\n",
       " (u'as', 77874),\n",
       " (u'are', 72731),\n",
       " (u'have', 72649),\n",
       " (u's', 72443),\n",
       " (u'your', 63192),\n",
       " (u'with', 60039),\n",
       " (u't', 59801),\n",
       " (u'if', 59174),\n",
       " (u'article', 57902),\n",
       " (u'was', 54862),\n",
       " (u'or', 53876),\n",
       " (u'but', 51469),\n",
       " (u'wikipedia', 46715),\n",
       " (u'page', 46352),\n",
       " (u'my', 45497),\n",
       " (u'an', 45224),\n",
       " (u'from', 41809),\n",
       " (u'by', 41630),\n",
       " (u'do', 40361),\n",
       " (u'can', 39789),\n",
       " (u'at', 39736),\n",
       " (u'about', 37335),\n",
       " (u'so', 36840),\n",
       " (u'me', 36745),\n",
       " (u'what', 35582),\n",
       " (u'there', 35494),\n",
       " (u'all', 31818),\n",
       " (u'has', 31067),\n",
       " (u'will', 30812),\n",
       " (u'please', 30209),\n",
       " (u'he', 29569),\n",
       " (u'would', 29547),\n",
       " (u'they', 29479),\n",
       " (u'no', 29465)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 words\n",
    "word_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'leaded', 1),\n",
       " (u'shemeet', 1),\n",
       " (u'pe\\u026al\\u0268n', 1),\n",
       " (u'hmmpff', 1),\n",
       " (u'qoyunli', 1),\n",
       " (u'thoroughfare', 1),\n",
       " (u'fradaulent', 1),\n",
       " (u'shty', 1),\n",
       " (u'proberly', 1),\n",
       " (u'pocketbook', 1),\n",
       " (u'mahakavyas', 1),\n",
       " (u'fudd', 1),\n",
       " (u'cryokinesis', 1),\n",
       " (u'wonk', 1),\n",
       " (u'sipopo', 1),\n",
       " (u'belembay', 1),\n",
       " (u'knisfo', 1),\n",
       " (u'onclelosse', 1),\n",
       " (u'pertecting', 1),\n",
       " (u'antivermins', 1),\n",
       " (u'warrig', 1),\n",
       " (u'ajna', 1),\n",
       " (u'talkapge', 1),\n",
       " (u'nepotising', 1),\n",
       " (u'rattner2', 1),\n",
       " (u'bratwurst', 1),\n",
       " (u'publicationthe', 1),\n",
       " (u'clarityafflicting', 1),\n",
       " (u'ornella', 1),\n",
       " (u'cronyn', 1),\n",
       " (u'australianist', 1),\n",
       " (u'chromate', 1),\n",
       " (u'ehlers', 1),\n",
       " (u'spanko', 1),\n",
       " (u'thurst', 1),\n",
       " (u'gnawing', 1),\n",
       " (u'bennies', 1),\n",
       " (u'spanky', 1),\n",
       " (u'as_of', 1),\n",
       " (u'branco', 1),\n",
       " (u'\\u65b0\\u64b0\\u59d3\\u6c0f\\u9332', 1),\n",
       " (u'accoutns', 1),\n",
       " (u'queensborough', 1),\n",
       " (u'commagene', 1),\n",
       " (u'personal_attacks_', 1),\n",
       " (u'psone', 1),\n",
       " (u'fapped', 1),\n",
       " (u'classsssssssss', 1),\n",
       " (u'morihiro', 1),\n",
       " (u'downstep', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bottom 10 words\n",
    "word_counts.most_common()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select words with more than 1 occurrence\n",
    "select = {k: v for k, v in word_counts.iteritems() if v > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87348"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign unique indexes to each word\n",
    "sorted_words = sorted(select.iteritems(), key=lambda (k, v): (v, k), reverse=True)\n",
    "word_indexes = {k: i for i, (k, _) in enumerate(sorted_words)}\n",
    "\n",
    "# save vocabulary\n",
    "import json\n",
    "with open('datasets/semantic_vocab.json', 'w') as f:\n",
    "    json.dump(word_indexes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes['damn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# semantic vector mapper\n",
    "def semantic_vector(text):\n",
    "    text = semantic_prepro(text)\n",
    "    vector = np.zeros((len(word_indexes),), dtype=np.float32)\n",
    "    for w in text.split():\n",
    "        ind = word_indexes.get(w)\n",
    "        if ind is not None:\n",
    "            vector[ind] = 1.\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\" ::::That's reasonable enough; I just saw the conflict on this one page and not on the rest.  I don't know details of it either, and if I had noticed the rest I probably would have been suspicious also (like Mav, below).  Also like mav, though, I probably would have noted why I reverted any changes on the article's talk page.  Best,   \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text\n",
    "example = comments['comment'][30]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate vector\n",
    "vector_words = semantic_vector(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87348,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_words[word_indexes['enough']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = semantic_prepro(example).split()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter vector\n",
    "\n",
    "We generate a list of 2-lettergram from alpha characters (a-z) then map a given text's characters to this list to produce the letter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import itertools\n",
    "\n",
    "# character 1-grams (letters, numbers and whitespace)\n",
    "chars = list(u\"\".join(comb) for comb in set(itertools.permutations(set(string.letters.lower()), 2)))\n",
    "char_indexes = {c: i for i, c in enumerate(sorted(chars))}\n",
    "\n",
    "# save letter vocab\n",
    "with open('datasets/letter_vocab.json', 'w') as f:\n",
    "    json.dump(char_indexes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'ab': 0,\n",
       " u'ac': 1,\n",
       " u'ad': 2,\n",
       " u'ae': 3,\n",
       " u'af': 4,\n",
       " u'ag': 5,\n",
       " u'ah': 6,\n",
       " u'ai': 7,\n",
       " u'aj': 8,\n",
       " u'ak': 9,\n",
       " u'al': 10,\n",
       " u'am': 11,\n",
       " u'an': 12,\n",
       " u'ao': 13,\n",
       " u'ap': 14,\n",
       " u'aq': 15,\n",
       " u'ar': 16,\n",
       " u'as': 17,\n",
       " u'at': 18,\n",
       " u'au': 19,\n",
       " u'av': 20,\n",
       " u'aw': 21,\n",
       " u'ax': 22,\n",
       " u'ay': 23,\n",
       " u'az': 24,\n",
       " u'ba': 25,\n",
       " u'bc': 26,\n",
       " u'bd': 27,\n",
       " u'be': 28,\n",
       " u'bf': 29,\n",
       " u'bg': 30,\n",
       " u'bh': 31,\n",
       " u'bi': 32,\n",
       " u'bj': 33,\n",
       " u'bk': 34,\n",
       " u'bl': 35,\n",
       " u'bm': 36,\n",
       " u'bn': 37,\n",
       " u'bo': 38,\n",
       " u'bp': 39,\n",
       " u'bq': 40,\n",
       " u'br': 41,\n",
       " u'bs': 42,\n",
       " u'bt': 43,\n",
       " u'bu': 44,\n",
       " u'bv': 45,\n",
       " u'bw': 46,\n",
       " u'bx': 47,\n",
       " u'by': 48,\n",
       " u'bz': 49,\n",
       " u'ca': 50,\n",
       " u'cb': 51,\n",
       " u'cd': 52,\n",
       " u'ce': 53,\n",
       " u'cf': 54,\n",
       " u'cg': 55,\n",
       " u'ch': 56,\n",
       " u'ci': 57,\n",
       " u'cj': 58,\n",
       " u'ck': 59,\n",
       " u'cl': 60,\n",
       " u'cm': 61,\n",
       " u'cn': 62,\n",
       " u'co': 63,\n",
       " u'cp': 64,\n",
       " u'cq': 65,\n",
       " u'cr': 66,\n",
       " u'cs': 67,\n",
       " u'ct': 68,\n",
       " u'cu': 69,\n",
       " u'cv': 70,\n",
       " u'cw': 71,\n",
       " u'cx': 72,\n",
       " u'cy': 73,\n",
       " u'cz': 74,\n",
       " u'da': 75,\n",
       " u'db': 76,\n",
       " u'dc': 77,\n",
       " u'de': 78,\n",
       " u'df': 79,\n",
       " u'dg': 80,\n",
       " u'dh': 81,\n",
       " u'di': 82,\n",
       " u'dj': 83,\n",
       " u'dk': 84,\n",
       " u'dl': 85,\n",
       " u'dm': 86,\n",
       " u'dn': 87,\n",
       " u'do': 88,\n",
       " u'dp': 89,\n",
       " u'dq': 90,\n",
       " u'dr': 91,\n",
       " u'ds': 92,\n",
       " u'dt': 93,\n",
       " u'du': 94,\n",
       " u'dv': 95,\n",
       " u'dw': 96,\n",
       " u'dx': 97,\n",
       " u'dy': 98,\n",
       " u'dz': 99,\n",
       " u'ea': 100,\n",
       " u'eb': 101,\n",
       " u'ec': 102,\n",
       " u'ed': 103,\n",
       " u'ef': 104,\n",
       " u'eg': 105,\n",
       " u'eh': 106,\n",
       " u'ei': 107,\n",
       " u'ej': 108,\n",
       " u'ek': 109,\n",
       " u'el': 110,\n",
       " u'em': 111,\n",
       " u'en': 112,\n",
       " u'eo': 113,\n",
       " u'ep': 114,\n",
       " u'eq': 115,\n",
       " u'er': 116,\n",
       " u'es': 117,\n",
       " u'et': 118,\n",
       " u'eu': 119,\n",
       " u'ev': 120,\n",
       " u'ew': 121,\n",
       " u'ex': 122,\n",
       " u'ey': 123,\n",
       " u'ez': 124,\n",
       " u'fa': 125,\n",
       " u'fb': 126,\n",
       " u'fc': 127,\n",
       " u'fd': 128,\n",
       " u'fe': 129,\n",
       " u'fg': 130,\n",
       " u'fh': 131,\n",
       " u'fi': 132,\n",
       " u'fj': 133,\n",
       " u'fk': 134,\n",
       " u'fl': 135,\n",
       " u'fm': 136,\n",
       " u'fn': 137,\n",
       " u'fo': 138,\n",
       " u'fp': 139,\n",
       " u'fq': 140,\n",
       " u'fr': 141,\n",
       " u'fs': 142,\n",
       " u'ft': 143,\n",
       " u'fu': 144,\n",
       " u'fv': 145,\n",
       " u'fw': 146,\n",
       " u'fx': 147,\n",
       " u'fy': 148,\n",
       " u'fz': 149,\n",
       " u'ga': 150,\n",
       " u'gb': 151,\n",
       " u'gc': 152,\n",
       " u'gd': 153,\n",
       " u'ge': 154,\n",
       " u'gf': 155,\n",
       " u'gh': 156,\n",
       " u'gi': 157,\n",
       " u'gj': 158,\n",
       " u'gk': 159,\n",
       " u'gl': 160,\n",
       " u'gm': 161,\n",
       " u'gn': 162,\n",
       " u'go': 163,\n",
       " u'gp': 164,\n",
       " u'gq': 165,\n",
       " u'gr': 166,\n",
       " u'gs': 167,\n",
       " u'gt': 168,\n",
       " u'gu': 169,\n",
       " u'gv': 170,\n",
       " u'gw': 171,\n",
       " u'gx': 172,\n",
       " u'gy': 173,\n",
       " u'gz': 174,\n",
       " u'ha': 175,\n",
       " u'hb': 176,\n",
       " u'hc': 177,\n",
       " u'hd': 178,\n",
       " u'he': 179,\n",
       " u'hf': 180,\n",
       " u'hg': 181,\n",
       " u'hi': 182,\n",
       " u'hj': 183,\n",
       " u'hk': 184,\n",
       " u'hl': 185,\n",
       " u'hm': 186,\n",
       " u'hn': 187,\n",
       " u'ho': 188,\n",
       " u'hp': 189,\n",
       " u'hq': 190,\n",
       " u'hr': 191,\n",
       " u'hs': 192,\n",
       " u'ht': 193,\n",
       " u'hu': 194,\n",
       " u'hv': 195,\n",
       " u'hw': 196,\n",
       " u'hx': 197,\n",
       " u'hy': 198,\n",
       " u'hz': 199,\n",
       " u'ia': 200,\n",
       " u'ib': 201,\n",
       " u'ic': 202,\n",
       " u'id': 203,\n",
       " u'ie': 204,\n",
       " u'if': 205,\n",
       " u'ig': 206,\n",
       " u'ih': 207,\n",
       " u'ij': 208,\n",
       " u'ik': 209,\n",
       " u'il': 210,\n",
       " u'im': 211,\n",
       " u'in': 212,\n",
       " u'io': 213,\n",
       " u'ip': 214,\n",
       " u'iq': 215,\n",
       " u'ir': 216,\n",
       " u'is': 217,\n",
       " u'it': 218,\n",
       " u'iu': 219,\n",
       " u'iv': 220,\n",
       " u'iw': 221,\n",
       " u'ix': 222,\n",
       " u'iy': 223,\n",
       " u'iz': 224,\n",
       " u'ja': 225,\n",
       " u'jb': 226,\n",
       " u'jc': 227,\n",
       " u'jd': 228,\n",
       " u'je': 229,\n",
       " u'jf': 230,\n",
       " u'jg': 231,\n",
       " u'jh': 232,\n",
       " u'ji': 233,\n",
       " u'jk': 234,\n",
       " u'jl': 235,\n",
       " u'jm': 236,\n",
       " u'jn': 237,\n",
       " u'jo': 238,\n",
       " u'jp': 239,\n",
       " u'jq': 240,\n",
       " u'jr': 241,\n",
       " u'js': 242,\n",
       " u'jt': 243,\n",
       " u'ju': 244,\n",
       " u'jv': 245,\n",
       " u'jw': 246,\n",
       " u'jx': 247,\n",
       " u'jy': 248,\n",
       " u'jz': 249,\n",
       " u'ka': 250,\n",
       " u'kb': 251,\n",
       " u'kc': 252,\n",
       " u'kd': 253,\n",
       " u'ke': 254,\n",
       " u'kf': 255,\n",
       " u'kg': 256,\n",
       " u'kh': 257,\n",
       " u'ki': 258,\n",
       " u'kj': 259,\n",
       " u'kl': 260,\n",
       " u'km': 261,\n",
       " u'kn': 262,\n",
       " u'ko': 263,\n",
       " u'kp': 264,\n",
       " u'kq': 265,\n",
       " u'kr': 266,\n",
       " u'ks': 267,\n",
       " u'kt': 268,\n",
       " u'ku': 269,\n",
       " u'kv': 270,\n",
       " u'kw': 271,\n",
       " u'kx': 272,\n",
       " u'ky': 273,\n",
       " u'kz': 274,\n",
       " u'la': 275,\n",
       " u'lb': 276,\n",
       " u'lc': 277,\n",
       " u'ld': 278,\n",
       " u'le': 279,\n",
       " u'lf': 280,\n",
       " u'lg': 281,\n",
       " u'lh': 282,\n",
       " u'li': 283,\n",
       " u'lj': 284,\n",
       " u'lk': 285,\n",
       " u'lm': 286,\n",
       " u'ln': 287,\n",
       " u'lo': 288,\n",
       " u'lp': 289,\n",
       " u'lq': 290,\n",
       " u'lr': 291,\n",
       " u'ls': 292,\n",
       " u'lt': 293,\n",
       " u'lu': 294,\n",
       " u'lv': 295,\n",
       " u'lw': 296,\n",
       " u'lx': 297,\n",
       " u'ly': 298,\n",
       " u'lz': 299,\n",
       " u'ma': 300,\n",
       " u'mb': 301,\n",
       " u'mc': 302,\n",
       " u'md': 303,\n",
       " u'me': 304,\n",
       " u'mf': 305,\n",
       " u'mg': 306,\n",
       " u'mh': 307,\n",
       " u'mi': 308,\n",
       " u'mj': 309,\n",
       " u'mk': 310,\n",
       " u'ml': 311,\n",
       " u'mn': 312,\n",
       " u'mo': 313,\n",
       " u'mp': 314,\n",
       " u'mq': 315,\n",
       " u'mr': 316,\n",
       " u'ms': 317,\n",
       " u'mt': 318,\n",
       " u'mu': 319,\n",
       " u'mv': 320,\n",
       " u'mw': 321,\n",
       " u'mx': 322,\n",
       " u'my': 323,\n",
       " u'mz': 324,\n",
       " u'na': 325,\n",
       " u'nb': 326,\n",
       " u'nc': 327,\n",
       " u'nd': 328,\n",
       " u'ne': 329,\n",
       " u'nf': 330,\n",
       " u'ng': 331,\n",
       " u'nh': 332,\n",
       " u'ni': 333,\n",
       " u'nj': 334,\n",
       " u'nk': 335,\n",
       " u'nl': 336,\n",
       " u'nm': 337,\n",
       " u'no': 338,\n",
       " u'np': 339,\n",
       " u'nq': 340,\n",
       " u'nr': 341,\n",
       " u'ns': 342,\n",
       " u'nt': 343,\n",
       " u'nu': 344,\n",
       " u'nv': 345,\n",
       " u'nw': 346,\n",
       " u'nx': 347,\n",
       " u'ny': 348,\n",
       " u'nz': 349,\n",
       " u'oa': 350,\n",
       " u'ob': 351,\n",
       " u'oc': 352,\n",
       " u'od': 353,\n",
       " u'oe': 354,\n",
       " u'of': 355,\n",
       " u'og': 356,\n",
       " u'oh': 357,\n",
       " u'oi': 358,\n",
       " u'oj': 359,\n",
       " u'ok': 360,\n",
       " u'ol': 361,\n",
       " u'om': 362,\n",
       " u'on': 363,\n",
       " u'op': 364,\n",
       " u'oq': 365,\n",
       " u'or': 366,\n",
       " u'os': 367,\n",
       " u'ot': 368,\n",
       " u'ou': 369,\n",
       " u'ov': 370,\n",
       " u'ow': 371,\n",
       " u'ox': 372,\n",
       " u'oy': 373,\n",
       " u'oz': 374,\n",
       " u'pa': 375,\n",
       " u'pb': 376,\n",
       " u'pc': 377,\n",
       " u'pd': 378,\n",
       " u'pe': 379,\n",
       " u'pf': 380,\n",
       " u'pg': 381,\n",
       " u'ph': 382,\n",
       " u'pi': 383,\n",
       " u'pj': 384,\n",
       " u'pk': 385,\n",
       " u'pl': 386,\n",
       " u'pm': 387,\n",
       " u'pn': 388,\n",
       " u'po': 389,\n",
       " u'pq': 390,\n",
       " u'pr': 391,\n",
       " u'ps': 392,\n",
       " u'pt': 393,\n",
       " u'pu': 394,\n",
       " u'pv': 395,\n",
       " u'pw': 396,\n",
       " u'px': 397,\n",
       " u'py': 398,\n",
       " u'pz': 399,\n",
       " u'qa': 400,\n",
       " u'qb': 401,\n",
       " u'qc': 402,\n",
       " u'qd': 403,\n",
       " u'qe': 404,\n",
       " u'qf': 405,\n",
       " u'qg': 406,\n",
       " u'qh': 407,\n",
       " u'qi': 408,\n",
       " u'qj': 409,\n",
       " u'qk': 410,\n",
       " u'ql': 411,\n",
       " u'qm': 412,\n",
       " u'qn': 413,\n",
       " u'qo': 414,\n",
       " u'qp': 415,\n",
       " u'qr': 416,\n",
       " u'qs': 417,\n",
       " u'qt': 418,\n",
       " u'qu': 419,\n",
       " u'qv': 420,\n",
       " u'qw': 421,\n",
       " u'qx': 422,\n",
       " u'qy': 423,\n",
       " u'qz': 424,\n",
       " u'ra': 425,\n",
       " u'rb': 426,\n",
       " u'rc': 427,\n",
       " u'rd': 428,\n",
       " u're': 429,\n",
       " u'rf': 430,\n",
       " u'rg': 431,\n",
       " u'rh': 432,\n",
       " u'ri': 433,\n",
       " u'rj': 434,\n",
       " u'rk': 435,\n",
       " u'rl': 436,\n",
       " u'rm': 437,\n",
       " u'rn': 438,\n",
       " u'ro': 439,\n",
       " u'rp': 440,\n",
       " u'rq': 441,\n",
       " u'rs': 442,\n",
       " u'rt': 443,\n",
       " u'ru': 444,\n",
       " u'rv': 445,\n",
       " u'rw': 446,\n",
       " u'rx': 447,\n",
       " u'ry': 448,\n",
       " u'rz': 449,\n",
       " u'sa': 450,\n",
       " u'sb': 451,\n",
       " u'sc': 452,\n",
       " u'sd': 453,\n",
       " u'se': 454,\n",
       " u'sf': 455,\n",
       " u'sg': 456,\n",
       " u'sh': 457,\n",
       " u'si': 458,\n",
       " u'sj': 459,\n",
       " u'sk': 460,\n",
       " u'sl': 461,\n",
       " u'sm': 462,\n",
       " u'sn': 463,\n",
       " u'so': 464,\n",
       " u'sp': 465,\n",
       " u'sq': 466,\n",
       " u'sr': 467,\n",
       " u'st': 468,\n",
       " u'su': 469,\n",
       " u'sv': 470,\n",
       " u'sw': 471,\n",
       " u'sx': 472,\n",
       " u'sy': 473,\n",
       " u'sz': 474,\n",
       " u'ta': 475,\n",
       " u'tb': 476,\n",
       " u'tc': 477,\n",
       " u'td': 478,\n",
       " u'te': 479,\n",
       " u'tf': 480,\n",
       " u'tg': 481,\n",
       " u'th': 482,\n",
       " u'ti': 483,\n",
       " u'tj': 484,\n",
       " u'tk': 485,\n",
       " u'tl': 486,\n",
       " u'tm': 487,\n",
       " u'tn': 488,\n",
       " u'to': 489,\n",
       " u'tp': 490,\n",
       " u'tq': 491,\n",
       " u'tr': 492,\n",
       " u'ts': 493,\n",
       " u'tu': 494,\n",
       " u'tv': 495,\n",
       " u'tw': 496,\n",
       " u'tx': 497,\n",
       " u'ty': 498,\n",
       " u'tz': 499,\n",
       " u'ua': 500,\n",
       " u'ub': 501,\n",
       " u'uc': 502,\n",
       " u'ud': 503,\n",
       " u'ue': 504,\n",
       " u'uf': 505,\n",
       " u'ug': 506,\n",
       " u'uh': 507,\n",
       " u'ui': 508,\n",
       " u'uj': 509,\n",
       " u'uk': 510,\n",
       " u'ul': 511,\n",
       " u'um': 512,\n",
       " u'un': 513,\n",
       " u'uo': 514,\n",
       " u'up': 515,\n",
       " u'uq': 516,\n",
       " u'ur': 517,\n",
       " u'us': 518,\n",
       " u'ut': 519,\n",
       " u'uv': 520,\n",
       " u'uw': 521,\n",
       " u'ux': 522,\n",
       " u'uy': 523,\n",
       " u'uz': 524,\n",
       " u'va': 525,\n",
       " u'vb': 526,\n",
       " u'vc': 527,\n",
       " u'vd': 528,\n",
       " u've': 529,\n",
       " u'vf': 530,\n",
       " u'vg': 531,\n",
       " u'vh': 532,\n",
       " u'vi': 533,\n",
       " u'vj': 534,\n",
       " u'vk': 535,\n",
       " u'vl': 536,\n",
       " u'vm': 537,\n",
       " u'vn': 538,\n",
       " u'vo': 539,\n",
       " u'vp': 540,\n",
       " u'vq': 541,\n",
       " u'vr': 542,\n",
       " u'vs': 543,\n",
       " u'vt': 544,\n",
       " u'vu': 545,\n",
       " u'vw': 546,\n",
       " u'vx': 547,\n",
       " u'vy': 548,\n",
       " u'vz': 549,\n",
       " u'wa': 550,\n",
       " u'wb': 551,\n",
       " u'wc': 552,\n",
       " u'wd': 553,\n",
       " u'we': 554,\n",
       " u'wf': 555,\n",
       " u'wg': 556,\n",
       " u'wh': 557,\n",
       " u'wi': 558,\n",
       " u'wj': 559,\n",
       " u'wk': 560,\n",
       " u'wl': 561,\n",
       " u'wm': 562,\n",
       " u'wn': 563,\n",
       " u'wo': 564,\n",
       " u'wp': 565,\n",
       " u'wq': 566,\n",
       " u'wr': 567,\n",
       " u'ws': 568,\n",
       " u'wt': 569,\n",
       " u'wu': 570,\n",
       " u'wv': 571,\n",
       " u'wx': 572,\n",
       " u'wy': 573,\n",
       " u'wz': 574,\n",
       " u'xa': 575,\n",
       " u'xb': 576,\n",
       " u'xc': 577,\n",
       " u'xd': 578,\n",
       " u'xe': 579,\n",
       " u'xf': 580,\n",
       " u'xg': 581,\n",
       " u'xh': 582,\n",
       " u'xi': 583,\n",
       " u'xj': 584,\n",
       " u'xk': 585,\n",
       " u'xl': 586,\n",
       " u'xm': 587,\n",
       " u'xn': 588,\n",
       " u'xo': 589,\n",
       " u'xp': 590,\n",
       " u'xq': 591,\n",
       " u'xr': 592,\n",
       " u'xs': 593,\n",
       " u'xt': 594,\n",
       " u'xu': 595,\n",
       " u'xv': 596,\n",
       " u'xw': 597,\n",
       " u'xy': 598,\n",
       " u'xz': 599,\n",
       " u'ya': 600,\n",
       " u'yb': 601,\n",
       " u'yc': 602,\n",
       " u'yd': 603,\n",
       " u'ye': 604,\n",
       " u'yf': 605,\n",
       " u'yg': 606,\n",
       " u'yh': 607,\n",
       " u'yi': 608,\n",
       " u'yj': 609,\n",
       " u'yk': 610,\n",
       " u'yl': 611,\n",
       " u'ym': 612,\n",
       " u'yn': 613,\n",
       " u'yo': 614,\n",
       " u'yp': 615,\n",
       " u'yq': 616,\n",
       " u'yr': 617,\n",
       " u'ys': 618,\n",
       " u'yt': 619,\n",
       " u'yu': 620,\n",
       " u'yv': 621,\n",
       " u'yw': 622,\n",
       " u'yx': 623,\n",
       " u'yz': 624,\n",
       " u'za': 625,\n",
       " u'zb': 626,\n",
       " u'zc': 627,\n",
       " u'zd': 628,\n",
       " u'ze': 629,\n",
       " u'zf': 630,\n",
       " u'zg': 631,\n",
       " u'zh': 632,\n",
       " u'zi': 633,\n",
       " u'zj': 634,\n",
       " u'zk': 635,\n",
       " u'zl': 636,\n",
       " u'zm': 637,\n",
       " u'zn': 638,\n",
       " u'zo': 639,\n",
       " u'zp': 640,\n",
       " u'zq': 641,\n",
       " u'zr': 642,\n",
       " u'zs': 643,\n",
       " u'zt': 644,\n",
       " u'zu': 645,\n",
       " u'zv': 646,\n",
       " u'zw': 647,\n",
       " u'zx': 648,\n",
       " u'zy': 649}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# character mapper\n",
    "def letter_vector(text):\n",
    "    vector = np.zeros((len(char_indexes),), dtype=np.float32)\n",
    "    for i in xrange(0, len(text), 2):\n",
    "        c = text[i:i + 2]\n",
    "        i = char_indexes.get(c)\n",
    "        if i is not None:\n",
    "            vector[i] = 1.\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_letters = letter_vector(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate features\n",
    "\n",
    "Finally, after computing each feature for each example, we concatenate them into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_vector = np.concatenate([vector_words, vector_letters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87998,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_words) + len(vector_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 15362 positives records (154 batches)\n",
      "saved 144324 negatives records (1444 batches)\n"
     ]
    }
   ],
   "source": [
    "def save_features(data, name):\n",
    "    features = []\n",
    "    batch_num = 0\n",
    "    for i, comment in enumerate(data, 1):\n",
    "        vs = semantic_vector(comment)\n",
    "        vl = letter_vector(comment)\n",
    "        v = np.concatenate([vs, vl])\n",
    "        features.append(v)\n",
    "        if i % 100 == 0:\n",
    "            batch_num += 1\n",
    "            np.save('datasets/processed/{}.{:04}.npy'.format(name, batch_num), np.vstack(features))\n",
    "            del features[:]\n",
    "    if features:\n",
    "        batch_num += 1\n",
    "        np.save('datasets/processed/{}.{:04}.npy'.format(name, batch_num), np.vstack(features))\n",
    "        del features[:]\n",
    "    print 'saved {} {} records ({} batches)'.format(i, name, batch_num)\n",
    "    \n",
    "# save positives\n",
    "save_features(comments.query('label')['comment'], 'positives')\n",
    "# save negatives\n",
    "save_features(comments.query('~label')['comment'], 'negatives')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
