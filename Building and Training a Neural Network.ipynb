{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a Neural Network\n",
    "\n",
    "In the notebook, we load a dataset of positives and negatives, split them into 80% for training, 10% for validation and 10% for testing. We then build a Multi-Layer Percepctron (MLP) Neural Network using Tensorflow and train it using the training dataset and subsequently evaluate it using the validation and testing datasets.\n",
    "\n",
    "Finally, we will use `tf.train.Saver` to save a checkpoint of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split the Dataset\n",
    "\n",
    "We first load the dataset files and split them into train (80%), validation (10%) and test (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(name, perc):\n",
    "    import glob\n",
    "    files = sorted(glob.glob('datasets/processed/{}.*'.format(name)))\n",
    "    np.random.shuffle(files)\n",
    "    files = files[:int(len(files) * perc)]\n",
    "    dataset = []\n",
    "    for f in files:\n",
    "        dataset.append(np.load(f))\n",
    "    return np.vstack(dataset)\n",
    "\n",
    "positives = load_dataset('positives', perc=.3)\n",
    "negatives = load_dataset('negatives', perc=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [N] 5760 [P] 3680\n",
      "Valid: [N] 720 [P] 460\n",
      "Test:  [N] 720 [P] 460\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data):\n",
    "    np.random.shuffle(data)\n",
    "    train_s = int(len(data) * .8)\n",
    "    test_s = int(len(data) * .1)\n",
    "    train = data[:train_s]\n",
    "    valid = data[train_s:train_s + test_s]\n",
    "    test = data[train_s + test_s:]\n",
    "    return train, valid, test\n",
    "\n",
    "train_p, valid_p, test_p = split_dataset(positives)\n",
    "train_n, valid_n, test_n = split_dataset(negatives)\n",
    "\n",
    "print 'Train: [N] {} [P] {}'.format(len(train_n), len(train_p))\n",
    "print 'Valid: [N] {} [P] {}'.format(len(valid_n), len(valid_p))\n",
    "print 'Test:  [N] {} [P] {}'.format(len(test_n), len(test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge positive and negative training inputs and labels and shuffle them\n",
    "\n",
    "def merge_datasets(negatives, positives):\n",
    "    labels_p = np.ones((len(positives), 1), dtype=np.float32)\n",
    "    labels_n = np.zeros((len(negatives), 1), dtype=np.float32)\n",
    "    merged = np.concatenate([negatives, positives], axis=0)\n",
    "    labels = np.concatenate([labels_n, labels_p], axis=0)\n",
    "    indexes = np.arange(len(merged))\n",
    "    np.random.shuffle(indexes)\n",
    "    return merged[indexes], labels[indexes]\n",
    "\n",
    "\n",
    "train, labels = merge_datasets(train_n, train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator function for yielding batches from a dataset\n",
    "\n",
    "def batch_looper(inputs, labels, batch_size, loop=False):\n",
    "    \"\"\"yield tuples of (batch_size, inputs) and (batch_size, labels)\"\"\"\n",
    "    i = 0\n",
    "    batch = []\n",
    "    while True:\n",
    "        for record in zip(inputs, labels):\n",
    "            batch.append(record)\n",
    "            i += 1\n",
    "            if i % batch_size == 0:\n",
    "                yield zip(*batch)\n",
    "                batch = []\n",
    "        if not loop:\n",
    "            yield zip(*batch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "labels\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "batches = batch_looper(train, labels, 5)\n",
    "b1, l1 = next(batches)\n",
    "print 'features'\n",
    "print np.vstack(b1)\n",
    "print 'labels'\n",
    "print np.vstack(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Neural Network\n",
    "\n",
    "We construct a MLP with two hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholders\n",
    "\n",
    "We define placeholders for our input features (`x`) and labels (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input size is equal to the size of our concatenated features\n",
    "input_size = len(train[0])\n",
    "# input vector\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, input_size))\n",
    "# labels vector\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameters\n",
    "\n",
    "Define the number of layers and their respective sizes as well as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HP = {\n",
    "    'layers': [(1000, tf.sigmoid), (900, tf.sigmoid)],\n",
    "    'lr': 0.005\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "\n",
    "We define two hidden layers (sizes `1000` and `900` respectively) and the output layer (size 1) and apply the `tf.sigmoid` activation in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_layer(input_tensor, input_size, output_size, activation, name):\n",
    "    W_init = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)\n",
    "    W = tf.get_variable(name='{}_W'.format(name), shape=(input_size, output_size),\n",
    "                        initializer=W_init, dtype=tf.float32)\n",
    "    b_init = tf.constant_initializer(value=0.1, dtype=tf.float32)\n",
    "    b = tf.get_variable(name='{}_b'.format(name), shape=(output_size,), initializer=b_init,\n",
    "                        dtype=tf.float32)\n",
    "    return activation(tf.matmul(input_tensor, W) + b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_input = x\n",
    "current_size = input_size\n",
    "layers = []\n",
    "\n",
    "for i, (layer_size, activation) in enumerate(HP['layers'], 1):\n",
    "    layer = init_layer(current_input, current_size, layer_size, activation, name='layer_{}'.format(i))\n",
    "    current_input = layer\n",
    "    current_size = layer_size\n",
    "    layers.append(layer)\n",
    "\n",
    "# output layer\n",
    "output = init_layer(layer, current_size, 1, tf.sigmoid, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss = tf.reduce_mean(tf.pow(output - y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summaries for Tensorboard\n",
    "current_epoch = tf.placeholder(tf.float32)\n",
    "sum_loss = tf.Variable(initial_value=0., trainable=False, dtype=tf.float32)\n",
    "add_loss = tf.assign_add(sum_loss, loss)\n",
    "mean_loss = sum_loss / current_epoch\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('mean_loss', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "We train using Gradient Decent with a learning rate of `0.005`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train step\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=HP['lr']).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a saver for saving the model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We start a new Tensorflow `Session` and initialize all variables (the weight and bias matrixes defined in each layer). We then iterate through batches from our training dataset and run the training step with each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a new tensorflow session with the default Graph\n",
    "session = tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "train_summary = tf.summary.FileWriter('./train', session.graph)\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Loss: 0.31048\n",
      "[2] Loss: 0.27416\n",
      "[3] Loss: 0.26139\n",
      "[4] Loss: 0.25563\n",
      "[5] Loss: 0.25179\n",
      "[6] Loss: 0.24979\n",
      "[7] Loss: 0.24697\n",
      "[8] Loss: 0.24483\n",
      "[9] Loss: 0.24377\n",
      "[10] Loss: 0.24293\n",
      "[11] Loss: 0.24299\n",
      "[12] Loss: 0.24099\n",
      "[13] Loss: 0.23922\n",
      "[14] Loss: 0.23900\n",
      "[15] Loss: 0.23818\n",
      "[16] Loss: 0.23652\n",
      "[17] Loss: 0.23565\n",
      "[18] Loss: 0.23428\n",
      "[19] Loss: 0.23493\n",
      "[20] Loss: 0.23397\n",
      "[21] Loss: 0.23331\n",
      "[22] Loss: 0.23320\n",
      "[23] Loss: 0.23293\n",
      "[24] Loss: 0.23219\n",
      "[25] Loss: 0.23186\n",
      "[26] Loss: 0.23189\n",
      "[27] Loss: 0.23116\n",
      "[28] Loss: 0.23101\n",
      "[29] Loss: 0.23086\n",
      "[30] Loss: 0.23079\n",
      "[31] Loss: 0.23067\n",
      "[32] Loss: 0.23057\n",
      "[33] Loss: 0.23008\n",
      "[34] Loss: 0.22989\n",
      "[35] Loss: 0.22923\n",
      "[36] Loss: 0.22893\n",
      "[37] Loss: 0.22887\n",
      "[38] Loss: 0.22833\n",
      "[39] Loss: 0.22784\n",
      "[40] Loss: 0.22753\n",
      "[41] Loss: 0.22740\n",
      "[42] Loss: 0.22713\n",
      "[43] Loss: 0.22689\n",
      "[44] Loss: 0.22638\n",
      "[45] Loss: 0.22592\n",
      "[46] Loss: 0.22574\n",
      "[47] Loss: 0.22555\n",
      "[48] Loss: 0.22546\n",
      "[49] Loss: 0.22506\n",
      "[50] Loss: 0.22471\n",
      "[51] Loss: 0.22454\n",
      "[52] Loss: 0.22457\n",
      "[53] Loss: 0.22415\n",
      "[54] Loss: 0.22394\n",
      "[55] Loss: 0.22373\n",
      "[56] Loss: 0.22338\n",
      "[57] Loss: 0.22328\n",
      "[58] Loss: 0.22322\n",
      "[59] Loss: 0.22287\n",
      "[60] Loss: 0.22248\n",
      "[61] Loss: 0.22251\n",
      "[62] Loss: 0.22218\n",
      "[63] Loss: 0.22169\n",
      "[64] Loss: 0.22133\n",
      "[65] Loss: 0.22097\n",
      "[66] Loss: 0.22107\n",
      "[67] Loss: 0.22095\n",
      "[68] Loss: 0.22058\n",
      "[69] Loss: 0.22066\n",
      "[70] Loss: 0.22046\n",
      "[71] Loss: 0.22018\n",
      "[72] Loss: 0.22000\n",
      "[73] Loss: 0.21994\n",
      "[74] Loss: 0.21964\n",
      "[75] Loss: 0.21964\n",
      "[76] Loss: 0.21944\n",
      "[77] Loss: 0.21952\n",
      "[78] Loss: 0.21945\n",
      "[79] Loss: 0.21942\n",
      "[80] Loss: 0.21931\n"
     ]
    }
   ],
   "source": [
    "train_batches = batch_looper(train, labels, 10, loop=True)\n",
    "total_epochs = 0\n",
    "for e in range(1, 80 + 1):\n",
    "    for i in range(1, 20 + 1):\n",
    "        total_epochs += 1\n",
    "        batch, lbl = next(train_batches)\n",
    "        soutput = session.run([merged, train_step, loss, add_loss, mean_loss],\n",
    "                              feed_dict={x: batch, y: lbl, current_epoch: total_epochs})\n",
    "        summary = soutput[0]\n",
    "        train_summary.add_summary(summary, total_epochs)\n",
    "    meanloss = soutput[-1]\n",
    "    print '[{}] Loss: {:.5f}'.format(e, meanloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "To evaluate accuracy, we score both negatives and positives using the trained model.\n",
    "\n",
    "We compute the accuracy by considering scores > 0.5 as positives (**1**) otherwise as negatives (**0**) and compare the result with the correct labels. The accuracy is the average number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(positives, negatives):\n",
    "    scores = []\n",
    "    labels = []\n",
    "    \n",
    "    pos_labels = np.ones((len(positives),), dtype=np.bool)\n",
    "    \n",
    "    # score positives\n",
    "    for batch, _ in batch_looper(positives, pos_labels, 100):\n",
    "        scores.extend(session.run(output, feed_dict={x: batch}).flatten())\n",
    "    labels.extend(pos_labels)\n",
    "    \n",
    "    neg_labels = np.zeros((len(negatives),), dtype=np.bool)\n",
    "    \n",
    "    # score negatives\n",
    "    for batch, _ in batch_looper(negatives, neg_labels, 100):\n",
    "        scores.extend(session.run(output, feed_dict={x: batch}).flatten())\n",
    "    labels.extend(neg_labels)\n",
    "    \n",
    "    scores = np.array(scores, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.bool)\n",
    "    \n",
    "    # compute accuracy\n",
    "    scores = (scores > .5)\n",
    "    accuracy = (scores == labels).mean()\n",
    "    \n",
    "    print 'Accuracy: {}'.format(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.699152542373\n"
     ]
    }
   ],
   "source": [
    "evaluate(valid_p, valid_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.708474576271\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_p, test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add placeholder and prediction op into collections for easy access on load\n",
    "tf.add_to_collection('inputs', x)\n",
    "tf.add_to_collection('predictor', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/mlp-1600'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session, './models/mlp', global_step=total_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
